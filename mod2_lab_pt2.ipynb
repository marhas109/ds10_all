{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4c5a21",
   "metadata": {},
   "source": [
    "## Lab Part 2: CNN Architecture Design Decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad807d",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "Discuss the following key architectural decisions you would make and why:\n",
    "\n",
    "1. What kernel sizes would you choose for different layers and why?\n",
    "2. How would you incorporate pooling layers and what benefits would they provide in this specific application?\n",
    "3. What considerations would guide your decisions about network depth (number of layers) and width (number of filters per layer)?\n",
    "4. How might you leverage transfer learning in this scenario, and what would be the advantages?\n",
    "\n",
    "Support your answers with reasoning that demonstrates your understanding of CNN principles and the practical constraints of mobile deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb54222",
   "metadata": {},
   "source": [
    "### Mark's Answer\n",
    "\n",
    "My approach to the architecture of this CNN model is based on the following assumptions:\n",
    "\n",
    "1. I am assuming that the dataset of labeled images is quite high-resolution but will be scaled to 256 x 256 for the training step\n",
    "2. I will err on the side of a large number of kernels because: \n",
    "    a. the model will need to identify a wide range of defect types on a variety of different materials/parts\n",
    "    b. the dataset is relatively large, somewhat reducing the risk of overfitting\n",
    "3. Finalizing model architecture will be an interative process-what I describe below my not be the optimal configuration but is a good first attempt\n",
    "\n",
    "I'm also loosely basing the architecture of this CNN model on a combination of the model in the module 2 example and the VGG-16 model developed by a team at the University of Oxford. \n",
    "\n",
    "#### Outline of the Architecture\n",
    "\n",
    "1. Input layer: 256 x 256 x 3\n",
    "---\n",
    "2. Convolution layer: 32 3x3 kernels, \n",
    "3. Activation layer: reLU\n",
    "4. Max pooling layer: 2x2, stride 2\n",
    "---\n",
    "5. Convolution layer: 64 3x3 kernels\n",
    "6. Activation layer: reLU\n",
    "7. Max pooling layer: 2x2, stride 2\n",
    "---\n",
    "8. Convolution layer: 128 3x3 kernels\n",
    "9. Activation layer: reLU\n",
    "10. Max pooling layer: 2x2, stride 2\n",
    "---\n",
    "11. Convolution layer: 256 3x3 filters\n",
    "12. Activation layer: reLU\n",
    "10. Max pooling layer: 2x2, stride 2\n",
    "---\n",
    "11. Flattening layer\n",
    "---\n",
    "12. Fully connected layer: 512 neurons, reLU activation\n",
    "13. Dropout regularization: 0.5 dropout rate\n",
    "14. Fully connected layer: 128 neurons, reLU activation\n",
    "15. Dropout regularization: 0.5 dropout rate\n",
    "---\n",
    "16. Final output layer: 1 neuron, sigmoid activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c34e87",
   "metadata": {},
   "source": [
    "#### Notes and Commentary\n",
    "\n",
    "##### Kernel sizes by layer\n",
    "\n",
    "I've included 4 convolution layers with the following kernel counts: 32, 64, 128, 256.\n",
    "\n",
    "- The kernel count increases to allow the model to detect increasingly complex features in the deeper layers (i.e. hierarchical feature learning)\n",
    "- Each convolution layer is followed by an activation layer to introduce nonlinearity and a max pooling layer to downscale feature dimensions\n",
    "- I selected 4 convolution layers to give the model reasonably high sensitivity to complex features\n",
    "    - This is intentionally between the module 2 example and the VGG model as I believe the parts images are more complex than the x-rays, but don't need the computation/horsepower of VGG\n",
    "\n",
    "##### Testing, Adjusting, and Tuning the Model\n",
    "\n",
    "After training and evaluating this model, I'd look for signs of over- and/or underfitting.\n",
    "\n",
    "- If the model were overfitting, I would first remove a convolution layer and re-evaluate, then reduce the kernel sizes if overfitting were still occuring\n",
    "    - Both of these steps would reduce the sensitivty of the model, lessening the chance of the model memorizing the training data\n",
    "- If the model were underfitting, I would first add a convolution layer (with accompanying activation and pooling layers), then increase the kernel sizes if underfitting continued \n",
    "    - These actions would increase the sensitivity of the model, allowing it to detect more complex features\n",
    "\n",
    "##### Transfer Learning\n",
    "\n",
    "I would be keen to use transfer learning in this scenario (i.e. reusing knowledge learned on another dataset). The most important part of implementing transfer learning would be finding a comparable dataset and a model that performed well on it.\n",
    "\n",
    "This would bring a few notable advantages:\n",
    "\n",
    "1. The model and its architecture should require less adjusting and tuning, since it would already have demonstrated strong performance on a similar task\n",
    "2. The risk of over- or underfitting would also be less for the same reason"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
